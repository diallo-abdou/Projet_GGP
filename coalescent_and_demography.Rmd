---
title: "Projet_GGP"
author: "Abdou, Antoine et Youna"
date: "2023-11-28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

packages <- c("coala","ape","ggplot2", "deSolve", "optimx","dplyr","sensitivity","tidyr","GGally")
A <- lapply(
  packages,
  FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      install.packages(x, dependencies = TRUE)
      library(x, character.only = TRUE)
    }
  }
)

```

# Plan












# Effet d'un changement de la taille de la population

Pour modifier la démographie, nous devons utiliser la fonction \textit{feat_size_change} qui nécessite deux paramètres : la taille de la population ancestrale donnée comme une fraction de la taille de la population actuelle, $N_0$ et le temps (dans le passé) auquel la population est modifiée. Le temps est compté en $2N_0$ générations.
Nous supposons que le taux de mutation par site est $u=10^{-8}$ ***(plutot, $u = 1$)***. Nous simulons 10 individus séquencés à 1000 loci, chaque loci ayant une longueur de 1000 pb.


1) Réglez les paramètres NPAST, TCHANGE et THETA pour simuler les scénarios suivants :


## Scénario 1

- Une population ancestrale de taille $N_{anc}=10000$ qui passe à $N_0=50000$ 10000 générations dans le passé.

### NPAST

 
Dans le contexte du scénario 1, le facteur de changement de taille de population est :

$$\text{Facteur de changement} = \frac{N_{anc}}{N_{0}} $$

Dans ce scénario :

$$ \text{Facteur de changement} = \frac{10,000}{50,000} = 0.2 $$

### TCHANGE
Calculons `TCHANGE` dans le scénario où la taille de la population ancestrale passe de $N_{anc} = 10,000$ à $N_0 = 50,000$ sur 10,000 générations passées :

$$ TCHANGE = \frac{\text{Nombre de générations passées}}{2N_0} $$

Dans le scénario 1 :
$$ TCHANGE = \frac{10,000}{2 \times 50,000} = 0.1 $$

Ainsi, `TCHANGE` est égal à $0.1$, indiquant que le changement de taille de population s'est produit il y a $0.1$ unité de $2N_0$ générations (ou 10,000 générations) dans le passé. 


```{r}
# Default values corresponds to scenarios 1
set.seed(123)

NPAST <- 0.2 
TCHANGE <- 0.1 
THETA <- 1

model <- coal_model(sample_size = 10,loci_number = 100,loci_length = 100,ploidy = 2)
model <- model <- model + feat_mutation(rate = THETA)
model <- model + feat_size_change(new_size = NPAST,time = TCHANGE,population = "all")
model <- model +
  sumstat_tajimas_d(name = "Dtaj") +
  sumstat_nucleotide_div(name = "pi") +
  sumstat_sfs(name="sfs") +
  sumstat_trees(name="trees")
sim <- simulate(model)

mean_pi <- mean(sim$pi)
se_pi <- sd(sim$pi)/sqrt(NLOCI)
mean_Dtaj <- mean(sim$Dtaj,na.rm = T) # Dtaj is not defined if there is no SNPs
se_Dtaj <- var(sim$Dtaj,na.rm = T)/sqrt(NLOCI)


obs <- data.frame(mean_pi = mean(sim$pi), 
                 se_pi = sd(sim$pi)/sqrt(NLOCI), 
                 mean_Dtaj = mean(sim$Dtaj,na.rm = T), 
                 se_Dtaj = var(sim$Dtaj,na.rm = T)/sqrt(NLOCI))

layout(matrix(c(1:4),2,2))
hist(sim$pi,main=paste("Mean pi =",round(mean_pi,2)," +/- ",round(1.96*se_pi,2)),breaks=30)
hist(sim$Dtaj,main=paste("Mean Dtaj =",round(mean_Dtaj,2),"+/-",round(1.96*se_Dtaj,2)),breaks=30)
barplot(sim$sfs, main="SFS")
tr <- read.tree(text = sim$trees[[1]][[1]])
plot(tr,main="Example of a genealogy")

```






## Scénario 2

- Une population ancestrale de taille $N_{anc}=200000$ qui chute à $N_0=50000$ 50000 générations dans le passé.

### NPAST

Dans le contexte du scénario 2, le facteur de changement de taille de population est :

$$\text{Facteur de changement} = \frac{N_{anc}}{N_{0}} $$

Dans ce scénario :

$$ \text{Facteur de changement} = \frac{200000}{50 000} = 4 $$

### TCHANGE

Calculons `TCHANGE` dans le scénario où la taille de la population ancestrale chute de $N_{anc} = 200,000$ à $N_0 = 50,000$ sur 50,000 générations passées, vous devez utiliser la formule fournie dans votre question originale. Le temps du changement (`TCHANGE`) est mesuré en unités de $2N_0$ générations.


$$ TCHANGE = \frac{\text{Nombre de générations passées}}{2N_0} $$

Dans ce scénario :
$$ TCHANGE = \frac{50,000}{2 \times 50,000} = 0.5 $$

Ainsi, `TCHANGE` est égal à 0.5, ce qui indique que le changement de taille de population s'est produit il y a 0.5 unité de $2N_0$ générations (ou 50,000 générations).



```{r}
# Default values corresponds to scenarios 2

NPAST <- 4
TCHANGE <- 50000/(2*50000)
THETA <- 1

model <- coal_model(sample_size = 100,loci_number = 1000,loci_length = 1000,ploidy = 2)
model <- model <- model + feat_mutation(rate = THETA)
model <- model + feat_size_change(new_size = NPAST,time = TCHANGE,population = "all")
model <- model +
  sumstat_tajimas_d(name = "Dtaj") +
  sumstat_nucleotide_div(name = "pi") +
  sumstat_sfs(name="sfs") +
  sumstat_trees(name="trees")
sim <- simulate(model)



mean_pi <- mean(sim$pi)
se_pi <- sd(sim$pi)/sqrt(NLOCI)
mean_Dtaj <- mean(sim$Dtaj,na.rm = T) # Dtaj is not defined if there is no SNPs
se_Dtaj <- var(sim$Dtaj,na.rm = T)/sqrt(NLOCI)

layout(matrix(c(1:4),2,2))
hist(sim$pi,main=paste("Mean pi =",round(mean_pi,2)," +/- ",round(1.96*se_pi,2)),breaks=30)
hist(sim$Dtaj,main=paste("Mean Dtaj =",round(mean_Dtaj,2),"+/-",round(1.96*se_Dtaj,2)),breaks=30)
barplot(sim$sfs, main="SFS")
tr <- read.tree(text = sim$trees[[1]][[1]])
plot(tr,main="Example of a genealogy")

plot(density(sim$Dtaj, na.rm = TRUE), 
      main = "Distribution of Tajiam's D")
```



2) Run the model and interpret the results

3) Additional question: write the code to compare the SFS under the two demographic scenarios and the constant population size secnario.


## ABC avec Scénario 1



```{r}

# Install and load the optimx package

# Your objective function to be minimized
estimation <- function(x) {

# Default values corresponds to scenarios 1

  
NPAST <- x[1]  
TCHANGE <- x[2] 
THETA <- 1

model <- coal_model(sample_size = 10,loci_number = 100,loci_length = 100,ploidy = 2)
model <- model <- model + feat_mutation(rate = THETA)
model <- model + feat_size_change(new_size = NPAST,time = TCHANGE,population = "all")
model <- model +
  sumstat_tajimas_d(name = "Dtaj") +
  sumstat_nucleotide_div(name = "pi") +
  sumstat_sfs(name="sfs") +
  sumstat_trees(name="trees")
sim <- simulate(model)

mean_pi <- mean(sim$pi)
se_pi <- sd(sim$pi)/sqrt(NLOCI)
mean_Dtaj <- mean(sim$Dtaj,na.rm = T) # Dtaj is not defined if there is no SNPs
se_Dtaj <- var(sim$Dtaj,na.rm = T)/sqrt(NLOCI)



  # Création d'un dataframe pour ggplot2
simu <- data.frame(mean_pi = mean(sim$pi), 
                 se_pi = sd(sim$pi)/sqrt(NLOCI), 
                 mean_Dtaj = mean(sim$Dtaj,na.rm = T), 
                 se_Dtaj = var(sim$Dtaj,na.rm = T)/sqrt(NLOCI))

  # Compare obs and simu
dist= sqrt ( sum  (   ( obs[,c("mean_pi", "mean_Dtaj")] / simu[,c("mean_pi", "mean_Dtaj")]  - 1 )^2)    )

  return(dist)
}

initial_guess <- c(0.2, 0.1)  # Replace with your own initial values

estimation(initial_guess)


# Rappel: 
## Scénario 1

# Une population ancestrale de taille $N_{anc}=10 000$ qui passe à $N_0=50 000$ 10 000 générations dans le passé.

#N0 =x[1]
#Nanc=x[2]
#NPAST <-  Nanc/N0
#TCHANGE <- x[3] # a revoir peut être



df_stock <- data.frame( 
                 Nanc = character(), 
                 TCHANGE = character(), 
                 
                 dist= character())
start_time100 <- Sys.time()

for (i in c(1:10000)) {
  Nanc <- runif(n = 1,min = 0.01,max = 0.7)
  TCHANGE <- runif(n = 1,min = 0.01,max = 0.9) 
  
  valeurs=c(Nanc,TCHANGE)
  
  df_i=data.frame(
                 Nanc = Nanc, 
                 TCHANGE = TCHANGE, 
                 dist= estimation(valeurs))
  
  df_stock=rbind(df_stock,df_i)
}

end_time100 <- Sys.time() 
execution_time <- end_time100 - start_time100; execution_time


df <- df_stock %>% arrange(dist)
top_100 <- head(df, 50)

ggpairs(top_100[,1:2])

summary(top_100)

for (i in 1:2){ 
plot(density(top_100[,i], na.rm = TRUE), 
      main = paste0("Distribution de :",names(top_100[,i]) ))
}


```


## Echantillonnage avec la méthode FAST
```{r}
# Avec FAST


borne_para <- list(list(min=0.01,max=10),
                   list(min=0.01,max=5))

nom=c("Nanc","TCHANGE")
# QUESTION b: Cas a 100 scenario
start_time100 <- Sys.time()

fast10000 <- fast99(model=NULL,
                      factors=nom,
                      n=10000,
                      q=rep("qunif",2),
                      q.arg=borne_para)





val= fast10000$X

plot(val$Nanc)
plot(val$TCHANGE)



start_time <- Sys.time()
fast_df_stock <- data.frame( 
                 Nanc = character(), 
                 TCHANGE = character(), 
                 dist= character())

for (i in 1:nrow(val)){
  
  
  df=data.frame(Nanc= val[i,1], 
                TCHANGE= val[i,2],
                dist=estimation(c(val[i,1],val[i,2]) ) )
  
  fast_df_stock = rbind(fast_df_stock,df)
  
}

end_time <- Sys.time() 
execution_time <- end_time - start_time; execution_time




fast_df <- fast_df_stock %>% arrange(dist)
fast_top_50 <- head(fast_df, 50)

ggpairs(fast_top_50[,1:2])

summary(fast_top_50)

for (i in 1:2){ 
plot(density(fast_top_50[,i], na.rm = TRUE), 
      main = paste0("Distribution de :",names(fast_top_50[,i]) ))
}
```

## Bottelneck

Création du jeu de la simulation observée. Taille de pop ancestrale identique à celle actuelle, mais chute de 90% lors du bottleneck

```{r}
THETA <- 1
NLOCI <- 100
model_bottleneck <- coal_model(20,NLOCI,NLOCI,ploidy = 2)+
  feat_mutation(rate = THETA)+
  feat_size_change(0.1, time = 0.25) +
  feat_size_change(1, time = 0.5)+
  sumstat_sfs("sfs")+
  sumstat_tajimas_d(name = "Dtaj") +
  sumstat_nucleotide_div(name = "pi")+
  sumstat_trees("trees")
sim1 <- simulate(model_bottleneck)
sim2 <- simulate(model_bottleneck)
tr <- read.tree(text = sim1$trees[[1]][[1]])
plot(tr,main="Example of a genealogy", direction = "downwards")

mean(sim1$Dtaj)
mean(sim2$Dtaj)
mean(sim1$pi)
mean(sim2$pi)

coal_time_temp <- rep(NA,100)
for (i in 1:NLOCI) {
  tr <- read.tree(text = sim2$trees[[i]][[1]])
  btimes=sum(branching.times(as.phylo(tr)))
  coal_time_temp[i] <- btimes
}
mean_t1 <- mean(coal_time_temp)
mean_t2 <- mean(coal_time_temp)

 obs <- data.frame(mean_pi = mean(sim$pi), 
                  se_pi = sd(sim$pi)/sqrt(NLOCI), 
                  mean_Dtaj = mean(sim$Dtaj,na.rm = T), 
                  se_Dtaj = var(sim$Dtaj,na.rm = T)/sqrt(NLOCI))
```
Fonction d'estimation pour un scénario de bottleneck. On souhaite estimer 2 paramètres: la durée du bottleneck et l'intensité du bottleneck.
```{r}

# Install and load the optimx package
library(deSolve)
library(ggplot2)
library(optimx)


# Your objective function to be minimized 
estimation_bottleneck <- function(x, obs) {

# Default values corresponds to scenarios 1

  
NPAST1 <-  x[1]
TCHANGE1 <- x[2] 
NPAST2 <-  x[3]
TCHANGE2 <- x[4] 

NLOCI <- 100
THETA <- 1

# model_bottleneck <- coal_model(20,NLOCI,NLOCI,ploidy = 2)+
#   feat_mutation(rate = THETA)+
#   feat_size_change(NPAST2, time = TCHANGE2) +
#   feat_size_change(NPAST1, time = TCHANGE1)+
#   sumstat_tajimas_d(name = "Dtaj") +
#   sumstat_nucleotide_div(name = "pi")+
#   sumstat_trees("trees")
model_bottleneck <- coal_model(20,NLOCI,NLOCI,ploidy = 2)+
  feat_mutation(rate = THETA)+
  feat_size_change(NPAST2, time = TCHANGE2) +
  feat_size_change(NPAST1, time = TCHANGE1)+
  sumstat_tajimas_d(name = "Dtaj") +
  sumstat_nucleotide_div(name = "pi")
sim <- simulate(model_bottleneck)

# mean_pi <- mean(sim$pi)
# se_pi <- sd(sim$pi)/sqrt(NLOCI)
# mean_Dtaj <- mean(sim$Dtaj,na.rm = T) # Dtaj is not defined if there is no SNPs
# se_Dtaj <- var(sim$Dtaj,na.rm = T)/sqrt(NLOCI)
# coal_time_temp <- rep(NA,100)
# for (i in 1:NLOCI) {
#   tr <- read.tree(text = sim$trees[[i]][[1]])
#   btimes=sum(branching.times(as.phylo(tr)))
#   coal_time_temp[i] <- btimes
# }


#   # Création d'un dataframe pour ggplot2
# simu <- data.frame(mean_pi = mean(sim$pi), 
#                  mean_Dtaj = mean(sim$Dtaj,na.rm = T),
#                  mean_coal_time <- mean(coal_time_temp, na.rm=T))
simu <- data.frame(mean_pi = mean(sim$pi), 
                 mean_Dtaj = mean(sim$Dtaj,na.rm = T))


  # Compare obs and simu
  dist = sqrt(sum((obs[,c("mean_pi", "mean_Dtaj")]/simu[,c("mean_pi", "mean_Dtaj")]-1)^2))
  return(dist)
}




# Rappel: 
## Scénario 1

# Une population ancestrale de taille $N_{anc}=10 000$ qui passe à $N_0=50 000$ 10 000 générations dans le passé.

#N0 =x[1]
#Nanc=x[2]
#NPAST <-  Nanc/N0
#TCHANGE <- x[3] # a revoir peut être

# Initial guess
initial_guess <- c(1, 0.5, 0.1,0.25)  # Replace with your own initial values

estimation_bottleneck(initial_guess,obs)

df_stock <- data.frame(NPAST1 = character(), 
                 TCHANGE1 = character(),
                 NPAST2 = character(), 
                 TCHANGE2 = character(), 
                 dist= character())
#start_time100 <- Sys.time()

for (i in c(1:1000)) {
  NPAST1 <- runif(n = 1,min = 0.1,max = 5)
  TCHANGE1 <- runif(n = 1,min = 0,max = 2)
  NPAST2 <- runif(n = 1,min = 0.1,max = NPAST1)
  TCHANGE2 <- runif(n = 1,min = 0,max = TCHANGE1) 
  
  valeurs=c(NPAST1,TCHANGE1,NPAST2,TCHANGE2)
  
  df_i=data.frame(NPAST1 = NPAST1, 
                 TCHANGE1 = TCHANGE1,
                 NPAST2 = NPAST2, 
                 TCHANGE2 = TCHANGE2, 
                 dist= estimation_bottleneck(valeurs))
  
  df_stock=rbind(df_stock,df_i)
}

#end_time100 <- Sys.time() 
#execution_time <- end_time100 - start_time100; execution_time

library(dplyr)
library(ggplot2)
library(tidyr)
library(GGally)

df <- df_stock %>% arrange(dist)
top_100 <- head(df, 50)

ggpairs(top_100[,1:4])

for (i in 1:4){ 
plot(density(top_100[,i], na.rm = TRUE), 
      main = paste0("Distribution de :",colnames(top_100)[i] ))
}
apply(top_100,2,min)
apply(top_100,2,max)
apply(df_stock,2,min)
apply(df_stock,2,max)
min(top_100[,1])

```

```{r}
#but : on fractionne nos nb_simul en sep part égales (si nb_simul = 1000 simul et sep = 10 on réalise 100 "sous simulations"). Pour une sous simulation t, on tire dans les lois uniformes dont les bornes correspondent aux valeurs min et max des meilleurs paramètres estimés pour la sous simulation t-1. Idée: rétricir les bornes à dans lesquelles on tire pour se rapprocher au fur et a mesure des meilleurs paramètres et gagner en précision.
estimation_V2 <- function(nb_simul, sep, obs, coupe){
  df_stock <- data.frame(NPAST1 = character(), 
                 TCHANGE1 = character(),
                 NPAST2 = character(), 
                 TCHANGE2 = character(), 
                 dist= character())
  for (k in 1:sep) {
    if(k==1){
      min_NPAST1 <- 0.1
      max_NPAST1 <- 5
      min_TCHANGE1 <- 0
      max_TCHANGE1 <- 2
      min_NPAST2 <- 0.1
      min_TCHANGE2 <- 0
    }
    for (i in 1:(nb_simul/sep)){
      NPAST1 <- runif(n = 1,min = min_NPAST1,max = max_NPAST1)
      TCHANGE1 <- runif(n = 1,min = min_TCHANGE1,max = max_TCHANGE1)
      NPAST2 <- runif(n = 1,min = min_NPAST2,max = NPAST1)
      TCHANGE2 <- runif(n = 1,min = min_TCHANGE2,max = TCHANGE1) 
  
      valeurs=c(NPAST1,TCHANGE1,NPAST2,TCHANGE2)
  
      df_i=data.frame(NPAST1 = NPAST1, 
                    TCHANGE1 = TCHANGE1,
                    NPAST2 = NPAST2, 
                    TCHANGE2 = TCHANGE2, 
                    dist= estimation_bottleneck(valeurs, obs))
  
      df_stock=rbind(df_stock,df_i)
    }
    df <- df_stock %>% arrange(dist)
    top <- head(df, ceiling(5*(nb_simul/sep)/100)) #10% les meilleures
    min_NPAST1 <- min(top[,1])
    max_NPAST1 <- max(top[,1])
    min_TCHANGE1 <- min(top[,2])
    max_TCHANGE1 <- max(top[,2])
    min_NPAST2 <- min(top[,3])
    min_TCHANGE2 <- min(top[,4])
  }
  if(coupe){
    df_stock <- df_stock %>% arrange(dist)
    top <- head(df_stock, ceiling(5*(nb_simul)/100)) #10% les meilleures
    return(top)
  }else{
    return(df_stock)
  }
}

estimation_V3 <- function(nb_simul, sep, obs, coupe){
  df_stock <- data.frame(NPAST1 = character(), 
                 TCHANGE1 = character(),
                 NPAST2 = character(), 
                 TCHANGE2 = character(), 
                 dist= character())
  for (k in 1:2) {
    if(k==1){
      min_NPAST1 <- 0.1
      max_NPAST1 <- 5
      min_TCHANGE1 <- 0
      max_TCHANGE1 <- 2
      min_NPAST2 <- 0.1
      min_TCHANGE2 <- 0
      
      for (i in 1:(nb_simul/sep)){
        NPAST1 <- runif(n = 1,min = min_NPAST1,max = max_NPAST1)
        TCHANGE1 <- runif(n = 1,min = min_TCHANGE1,max = max_TCHANGE1)
        NPAST2 <- runif(n = 1,min = min_NPAST2,max = NPAST1)
        TCHANGE2 <- runif(n = 1,min = min_TCHANGE2,max = TCHANGE1) 
  
        valeurs=c(NPAST1,TCHANGE1,NPAST2,TCHANGE2)
  
        df_i=data.frame(NPAST1 = NPAST1, 
                    TCHANGE1 = TCHANGE1,
                    NPAST2 = NPAST2, 
                    TCHANGE2 = TCHANGE2, 
                    dist= estimation_bottleneck(valeurs, obs))
  
        df_stock=rbind(df_stock,df_i)
      }
      df <- df_stock %>% arrange(dist)
      top <- head(df, ceiling(5*(nb_simul/sep)/100)) #10% les meilleures
      min_NPAST1 <- min(top[,1])
      max_NPAST1 <- max(top[,1])
      min_TCHANGE1 <- min(top[,2])
      max_TCHANGE1 <- max(top[,2])
      min_NPAST2 <- min(top[,3])
      min_TCHANGE2 <- min(top[,4])
    }else{
      for (i in 1:(nb_simul-nb_simul/sep)){
        NPAST1 <- runif(n = 1,min = min_NPAST1,max = max_NPAST1)
        TCHANGE1 <- runif(n = 1,min = min_TCHANGE1,max = max_TCHANGE1)
        NPAST2 <- runif(n = 1,min = min_NPAST2,max = NPAST1)
        TCHANGE2 <- runif(n = 1,min = min_TCHANGE2,max = TCHANGE1) 
  
        valeurs=c(NPAST1,TCHANGE1,NPAST2,TCHANGE2)
  
        df_i=data.frame(NPAST1 = NPAST1, 
                    TCHANGE1 = TCHANGE1,
                    NPAST2 = NPAST2, 
                    TCHANGE2 = TCHANGE2, 
                    dist= estimation_bottleneck(valeurs, obs))
  
        df_stock=rbind(df_stock,df_i)
      }
    }
  }
  if(coupe){
    df_stock <- df_stock %>% arrange(dist)
    top <- head(df_stock, ceiling(5*(nb_simul)/100)) #10% les meilleures
    return(top)
  }else{
    return(df_stock)
  }
}


library(reshape)
#V3 où on estime 100 jeux de data et on fait la procédure si dessus pour les 100 jeux
bottleneck_V3 <- function(nb_obs, nb_simul, sep, param=c(1,0.5,0.1,0.25), coupe, start, method, seed){
  #paramètres pour les nb_obs estimations observés
  NPAST1 <-  param[1]
  TCHANGE1 <- param[2] 
  NPAST2 <-  param[3]
  TCHANGE2 <- param[4] 
  NLOCI <- 100
  THETA <- 1
  
  big_data <- data.frame()
  data_obs <- data.frame()
  for (j in start:(nb_obs)) {
    #if(j%in%seq(nb_obs/10,nb_obs,10)){
    #  print(paste0(j/nb_obs*100, "%"))
    #}
    #simulation d'un jeu de données obs
    model_bottleneck <- coal_model(20,NLOCI,NLOCI,ploidy = 2)+
  feat_mutation(rate = THETA)+
  feat_size_change(NPAST2, time = TCHANGE2) +
  feat_size_change(NPAST1, time = TCHANGE1)+
  sumstat_tajimas_d(name = "Dtaj") +
  sumstat_nucleotide_div(name = "pi")+
      sumstat_trees("trees")
    if(seed){
      set.seed(j)
    }
    sim <- simulate(model_bottleneck)
    
    coal_time_temp <- rep(NA,100)
for (i in 1:NLOCI) {
  tr <- read.tree(text = sim$trees[[i]][[1]])
  btimes=sum(branching.times(as.phylo(tr)))
  coal_time_temp[i] <- btimes
}

    obs <- data.frame(mean_pi = mean(sim$pi), 
                      mean_Dtaj = mean(sim$Dtaj,na.rm = T),
                      mean_coal_time = mean(coal_time_temp, na.rm=T))
    data_obs <- rbind(data_obs,obs)
    # simulation de nb_simul jeux de données simulés dont on va déterminer les paramètres
    if(method=="V2"){
      results <- estimation_V2(nb_simul,sep,obs,coupe)
    }
    if(method=="V3"){
      results <- estimation_V3(nb_simul,sep,obs,coupe)
    }
    #results <- estimation_V2(nb_simul,sep,obs,coupe)
    big_data <- rbind(big_data,results)
  }
  if(coupe){
    big_data$obs <- melt(sapply(start:nb_obs, rep, ceiling(nb_simul*0.05)))$value
    return(list(big_data, data_obs))
  }else{
    big_data$obs <- melt(sapply(start:nb_obs, rep, nb_simul))$value
  return(list(big_data, data_obs))
  }
}

```

```{r}
results <- estimation_V2(100,1, obs, coupe=F)
```

```{r}
results1 <- bottleneck_V3(1,10000,10, coupe = T, start = 1)
results1_bis <- bottleneck_V3(1,10000,10, coupe = T, start = 1, method = "V3")
results1_bis2 <- bottleneck_V3(1,10000,10, coupe = T, start = 1, method = "V2", seed = F)
results2 <- bottleneck_V3(2,10000,10, coupe = T, start = 2, method = "V2", seed = F)
results3 <- bottleneck_V3(3,10000,10, coupe = T, start = 3, method = "V2", seed = F)
results4 <- bottleneck_V3(4,10000,10, coupe = T, start = 4, method = "V2", seed = F)
results5 <- bottleneck_V3(5,10000,10, coupe = T, start = 5, method = "V2", seed = F)
results6 <- bottleneck_V3(6,10000,10, coupe = T, start = 6, method = "V2", seed = F)
results7 <- bottleneck_V3(7,10000,10, coupe = T, start = 7, method = "V2", seed = F)
results8 <- bottleneck_V3(8,10000,10, coupe = T, start = 8, method = "V2", seed = F)
results9 <- bottleneck_V3(9,100,1, coupe = T, start = 9)
results10 <- bottleneck_V3(10,100,1, coupe = T, start = 10)
#results <- bottleneck_V3(100,10,1, coupe = F)
```

```{r}
results <- NULL
for (i in 11:30) {
  results <- bottleneck_V3(i,10000,10, coupe = T, start = i, method = "V2", seed = F)
  sim <- results[[1]]
  obs <- results[[2]]
  titre_sim <- paste0("estimations pour l'observation", i,".csv")
  titre_obs <- paste0("observation", i,".csv")
  write.table(sim,titre_sim)
  write.table(obs,titre_obs)
  rm(results)
}

```

```{r}
tab_tot_simul <- rbind(results7[[1]],
                 results8[[1]])

tab_tot_obs <- rbind(results7[[2]],
                 results8[[2]])

df <- tab_tot_simul %>% arrange(dist)
top100 <- head(df, 100)
write.table(tab_tot_simul,"estimations pour 7 et 8 obs.csv")
write.table(tab_tot_obs,"observations (7 à 8).csv")


df <- results1[[1]] %>% arrange(dist)
df_bis <- results1_bis[[1]] %>% arrange(dist)
df_bis2 <- results1_bis2[[1]] %>% arrange(dist)
top_100 <- head(df, 100)
top_100_bis <- head(df, 100)
top_100_bis2 <- head(df, 100)

for (i in 1:4){ 
  print(ggplot(top_100)+
  geom_histogram(aes(x=top_100[,i], y=after_stat(density)), position = "identity", bins = 15)+
  geom_density(aes(x=top_100[,i]))+
  labs(title = paste0("Distribution de ",colnames(top_100)[i]))+
  geom_vline(xintercept = mean(top_100[,i]), color='red'))
}
for (i in 1:4){ 
  print(ggplot(top_100_bis)+
  geom_histogram(aes(x=top_100_bis[,i], y=after_stat(density)), position = "identity", bins = 15)+
  geom_density(aes(x=top_100_bis[,i]))+
  labs(title = paste0("Distribution de ",colnames(top_100_bis)[i]))+
  geom_vline(xintercept = mean(top_100_bis[,i]), color='red'))
}
for (i in 1:4){ 
  print(ggplot(top100)+
  geom_histogram(aes(x=top100[,i], y=after_stat(density)), position = "identity", bins = 15)+
  geom_density(aes(x=top100[,i]))+
  labs(title = paste0("Distribution de ",colnames(top100)[i]))+
  geom_vline(xintercept = mean(top100[,i]), color='red'))
}
for (i in 1:4){ 
  print(ggplot(results[[1]])+
  geom_histogram(aes(x=results[[1]][,i], y=after_stat(density)), position = "identity", bins = 15)+
  geom_density(aes(x=results[[1]][,i]))+
  labs(title = paste0("Distribution de ",colnames(results[[1]])[i]))+
  geom_vline(xintercept = mean(results[[1]][,i]), color='red'))
}

mean_mean <- data.frame()
for (i in 1:length(unique(results[[1]]$obs))) {
  subdata <- results[[1]][which(results[[1]]$obs==i),]
  mean_mean <- rbind(mean_mean,apply(subdata,2,mean))
}
colnames(mean_mean) <- colnames(results[[1]])
ecart_entre_obs <- apply(mean_mean,2,sd)

#compare aux 30 obs
ecart_entre_observes <- apply(results[[2]],2,sd)
ecart_entre_obs
ecart_entre_observes

mean(top_100$dist)
sd(top_100$dist)
```


```{r}
#T1-T2 pour avoir la distribution du temps du bottleneck
results[[1]]$duree <- results[[1]]$TCHANGE1-results[[1]]$TCHANGE2
results[[1]]$intensite <- results[[1]]$NPAST1/results[[1]]$NPAST2
top_100$duree <- top_100$TCHANGE1-top_100$TCHANGE2
top_100$intensite <- top_100$NPAST1/top_100$NPAST2

for (i in 7:8){ 
  print(ggplot(top_100)+
  geom_histogram(aes(x=top_100[,i], y=after_stat(density)), position = "identity", bins = 15)+
  geom_density(aes(x=top_100[,i]))+
  labs(title = paste0("Distribution de ",colnames(top_100)[i]))+
  geom_vline(xintercept = mean(top_100[,i]), color='red'))
}
for (i in 7:8){ 
  print(ggplot(results[[1]])+
  geom_histogram(aes(x=results[[1]][,i], y=after_stat(density)), position = "identity", bins = 15)+
  geom_density(aes(x=results[[1]][,i]))+
  labs(title = paste0("Distribution de ",colnames(results[[1]])[i]))+
  geom_vline(xintercept = mean(results[[1]][,i]), color='red'))
}
```

```{r}
data1_sim <- read.csv("estimations pour 1 à 5 obs.csv", sep = " ")
data2_sim <- read.csv("estimations pour 5 et 6 obs.csv", sep = " ")
data3_sim <- read.csv("estimations pour 7 et 8 obs.csv", sep = " ")
data4_sim <- read.csv("estimations pour l'observation9.csv", sep = " ")
data5_sim <- read.csv("estimations pour l'observation10.csv", sep = " ")

data1_obs <- read.csv("observations (1 à 5).csv", sep = " ")
data2_obs <- read.csv("observations (5 à 6).csv", sep = " ")
data3_obs <- read.csv("observations (7 à 8).csv", sep = " ")
data4_obs <- read.csv("observation9.csv", sep = " ")
data5_obs <- read.csv("observation10.csv", sep = " ")


simul <- rbind(data1_sim,
               data2_sim,
               data3_sim,
               data4_sim,
               data5_sim)
obs <- rbind(data1_obs,
               data2_obs,
               data3_obs,
               data4_obs,
               data5_obs)

best1 <- data.frame()
for (i in 1:length(unique(simul$obs))) {
  sub <- simul[which(simul$obs==i),]
  classe <- sub %>% arrange(dist)
  best <- head(classe,100)
  best1 <- rbind(best1,best)
}
classe <- best1 %>% arrange(dist)
top100 <- head(classe,100)

mean_mean <- c()
sd_sd <- c()
for (i in 1:length(unique(best1$obs))) {
  mean_mean[i] <- mean(best1[which(best1$obs==i),]$dist)
  sd_sd[i] <- sd(best1[which(best1$obs==i),]$dist)
}
big_mean <- mean(mean_mean)
big_sd <- sd(sd_sd)

```
```{r}
plot(density(top_100[,i], na.rm = TRUE), 
      main = paste0("Distribution de :",names(top_100[,i]) ))


prior_min <- c(0.1,0,0,0)
prior_max <- c(5,2,5,2)
for (i in 1:4){ 
  print(ggplot(top100)+
  geom_histogram(aes(x=top100[,i], y=after_stat(density)), position = "identity", bins = 15, alpha=0.7)+
  geom_density(aes(x=top100[,i]))+
  geom_density(aes(x=runif(100,prior_min[i],prior_max[i])), color='green')+
  labs(title = paste0("Distribution de ",colnames(top100)[i]))+
  geom_vline(xintercept = mean(top100[,i]), color='red')+
  theme_classic())
}
```
